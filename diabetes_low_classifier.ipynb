{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl  http://nightscout-data:3001/api/dumpcsv/ -o ../../data/nightscout/data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../data/nightscout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir='../../data/nightscout'\n",
    "fname = os.path.join(data_dir,'data.csv')\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "\n",
    "print(header, len(lines))\n",
    "print(lines[0])\n",
    "\n",
    "glucose_index = 0\n",
    "keep_from_index = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "len_of_each_line = len(lines[1].split(','))\n",
    "\n",
    "data = np.zeros((len(lines),(len_of_each_line-keep_from_index)))\n",
    "\n",
    "deletes = []\n",
    "for i,l in enumerate(lines):\n",
    "    if l!='':\n",
    "        data[i,:]  = [float(x) for x in l.split(',')[keep_from_index:]]\n",
    "    else:\n",
    "        deletes.append(i)\n",
    "        print(\"delete \",i)\n",
    "print(deletes)\n",
    "data = np.delete(data,deletes, axis=0)\n",
    "print(\"data sanity check\", data[-20:])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"backup data in separate object\")\n",
    "datab = np.copy(data)\n",
    "print(datab[108:115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_low_data(low_data_map, index, data_ = datab, num_before=24, num_after=5, lookahead=2):\n",
    "    print(\"looking up data for low_data_map\", low_data_map[index], \"raw data index is \", low_data_map[index][1] )\n",
    "    return data_[low_data_map[index][1]-num_before-lookahead : low_data_map[index][1]+num_after]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up = True\n",
    "low_count = 0\n",
    "low_index = []\n",
    "low_data_ = []\n",
    "low_data = []\n",
    "low_data_map = []\n",
    "normal_data = []\n",
    "low_threshold = 80\n",
    "'''\n",
    "The way this works is we'll use measures_before to predict. but lookahead is the gap between the last measure_before and the actual low event\n",
    "so this is handy if you want to predict based on the last 2 hours you have a low in the next 10 minutes, \n",
    "you would set lookahead to 2 and measure before to 24\n",
    "'''\n",
    "measures_before = 24\n",
    "lookahead = 2\n",
    "low_data_index = 0\n",
    "for index, i in enumerate(datab):\n",
    "    if i[0] <= low_threshold and up:\n",
    "        low_count+=1\n",
    "        up = False\n",
    "        low_index.append(index)\n",
    "        if index > measures_before + lookahead:\n",
    "            low_data_.append(datab[index-measures_before-lookahead:index-lookahead])\n",
    "            low_data_map.append([low_data_index, index])\n",
    "            low_data_index+=1\n",
    "\n",
    "    if i[0] >low_threshold and not up:\n",
    "        up = True\n",
    "\n",
    "continuous = 0\n",
    "for index, i in enumerate(datab):\n",
    "    if i[0] <=low_threshold :\n",
    "        continuous = 0\n",
    "    if i[0] > low_threshold :\n",
    "        continuous+=1\n",
    "    if continuous > measures_before:\n",
    "        normal_data.append(datab[index-measures_before-lookahead:index-lookahead])\n",
    "\n",
    "#adding index to low_data so we can inspect it later\n",
    "# print(len(low_data), low_data_[0])\n",
    "# print(\"map:\", low_data_map[0])\n",
    "# print(datab[low_data_map[0][1]-5:low_data_map[0][1]])\n",
    "# print(low_data_map)\n",
    "np.random.shuffle(low_data_map)\n",
    "np.random.shuffle(normal_data)\n",
    "for i in range(len(low_data_map)):\n",
    "    low_data.append(low_data_[low_data_map[i][0]])\n",
    "# del low_data_\n",
    "\n",
    "print(\"these should match if the mapping is correct.\")\n",
    "print(low_data_map[2])\n",
    "print(low_data[2].shape, low_data[2])\n",
    "print(\"ORIGINAL: \\n\", get_original_low_data(low_data_map, 2, data_=datab, num_before=measures_before+lookahead))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#build the data set\n",
    "\n",
    "splits = [.75,.2,.05]\n",
    "norm_multiplier = 2\n",
    "if(sum(splits) != 1):\n",
    "    raise ValueError('splits must add up to 1')\n",
    "\n",
    "train_low_i = round(len(low_data)*splits[0])\n",
    "val_low_i = [train_low_i, train_low_i+round(len(low_data)*splits[1])]\n",
    "test_low_i = [val_low_i[1], val_low_i[1]+round(len(low_data)*splits[2])]\n",
    "\n",
    "train_norm_i = train_low_i * norm_multiplier\n",
    "val_norm_i = [train_norm_i, val_low_i[1] * norm_multiplier]\n",
    "test_norm_i = [val_norm_i[1], test_low_i[1] * norm_multiplier]\n",
    "\n",
    "#training data\n",
    "train_data_low = np.array(low_data[:train_low_i])\n",
    "train_data_norm = np.array(normal_data[:train_norm_i])\n",
    "\n",
    "train_labels_low = np.full((train_low_i,), 1)\n",
    "train_labels_norm = np.full((train_norm_i,), 0)\n",
    "\n",
    "train_data = np.concatenate((train_data_low, train_data_norm), axis=0)\n",
    "train_labels = np.concatenate((train_labels_low, train_labels_norm), axis=0)\n",
    "#validation data\n",
    "val_data_low = np.array(low_data[val_low_i[0]:val_low_i[1]])\n",
    "val_data_norm = np.array(normal_data[val_norm_i[0]:val_norm_i[1]])\n",
    "\n",
    "val_labels_low = np.full((len(val_data_low),), 1)\n",
    "val_labels_norm = np.full((len(val_data_norm),), 0)\n",
    "\n",
    "val_data = np.concatenate((val_data_low, val_data_norm), axis=0)\n",
    "val_labels = np.concatenate((val_labels_low, val_labels_norm), axis=0)\n",
    "#test data\n",
    "test_data_low = np.array(low_data[test_low_i[0]:test_low_i[1]])\n",
    "test_data_norm = np.array(normal_data[test_norm_i[0]:test_norm_i[1]])\n",
    "test_labels_low = np.full((len(test_data_low),), 1)\n",
    "test_labels_norm = np.full((len(test_data_norm),), 0)\n",
    "test_data = np.concatenate((test_data_low, test_data_norm), axis=0)\n",
    "test_labels = np.concatenate((test_labels_low, test_labels_norm), axis=0)\n",
    "print(\"*** LOWS ***\")\n",
    "print(\"training low data indexes: \",\"000\",\"->\", train_low_i)\n",
    "print(\"validati low data indexes: \", val_low_i[0],\"->\",val_low_i[1])\n",
    "print(\"testing  low data indexes: \", test_low_i[0],\"->\",test_low_i[1])\n",
    "\n",
    "print(\"*** NORMS ***\")\n",
    "print(\"training norm data indexes: \",\"000\",\"->\", train_norm_i)\n",
    "print(\"validati norm data indexes: \", val_norm_i[0],\"->\",val_norm_i[1])\n",
    "print(\"testing  norm data indexes: \", test_norm_i[0],\"->\",test_norm_i[1])\n",
    "\n",
    "print(\"*** COMBINED ***\")\n",
    "print(\"training: \", train_data.shape, train_labels.shape)\n",
    "print(\"validati: \", val_data.shape, val_labels.shape)\n",
    "print(\"test.   : \", test_data.shape, test_labels.shape)\n",
    "# train_data = np.concatenate((train_data_low, train_data_normal), axis=0)\n",
    "# train_labels = np.concatenate((train_labels_low, train_labels_normal), axis=0)\n",
    "\n",
    "# print(train_data[10], train_labels[10], \"\\nOG:\\n\", get_original_low_data(low_data_map, 10))\n",
    "# train_data_n = normal_data[]\n",
    "\n",
    "\n",
    "# validation_data = np.array([(low_data[round(len(low_data)*.75):], np.full((len(low_data)-len(training_data[0]),), 1))])\n",
    "# print(len(low_data[:round(len(low_data)*.75)]))\n",
    "# print(len( np.full((round(len(low_data)*.75),), 1)))\n",
    "# print(len())\n",
    "# training_data  = np.concatenate((training_data, normal_data[:len(training_data)*2]), axis=0)\n",
    "# validation_data  = np.concatenate((validation_data, normal_data[:len(validation_data)*2]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize/scale data\n",
    "#we will create standard scaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_data_flat = train_data.reshape(-1,3)\n",
    "# print(train_data_flat)\n",
    "# print(train_data[0])\n",
    "scaler.fit(train_data_flat)\n",
    "print(\"scaler: std: \", scaler.var_, \"\\n std: \", data[:25000].std(axis=0))\n",
    "print(\"\\nscaler: mean: \", scaler.mean_, \"\\n mean: \", data[:25000].mean(axis=0))\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = scaler.transform(train_data[i])\n",
    "for i in range(len(val_data)):\n",
    "    val_data[i] = scaler.transform(val_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data summary\")\n",
    "print(train_data.shape, \"\\n\", train_data)\n",
    "print(val_data.shape, \"\\n\", val_data)\n",
    "print(test_data.shape, \"\\n\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploratory model with train/val split. find poitn of overfit\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,  return_sequences=True, input_shape=(None, train_data.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam')\n",
    "history = model.fit(train_data, train_labels,epochs=100, validation_data=(val_data, val_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph loss/val loss\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    historydf.plot(ylim=(0, max(1, historydf.values.max())))\n",
    "    loss = history.history['loss'][-1]\n",
    "    acc = history.history['accuracy'][-1]\n",
    "    plt.title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))\n",
    "\n",
    "def plot_loss(history):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    historydf.plot(ylim=(0, historydf.values.max()))\n",
    "    plt.title('Loss: %.3f' % history.history['loss'][-1])\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dropout network, exploratory with train/val/test\n",
    "model_with_dropout = Sequential()\n",
    "#need reset_after for GRU compatibility with tfjs as this model will run in node\n",
    "model_with_dropout.add(layers.GRU(32, reset_after=False,  return_sequences=True, input_shape=(None, train_data.shape[-1])))\n",
    "model_with_dropout.add(layers.GRU(64, reset_after=False, dropout=0.2, activation='relu'))\n",
    "model_with_dropout.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model_with_dropout.compile(loss=\"binary_crossentropy\", optimizer=Adam(.0005))\n",
    "history_with_dropout = model_with_dropout.fit(train_data, train_labels,batch_size=2, epochs=100, validation_data=(val_data, val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRODUCTION network\n",
    "\n",
    "#use hyperparams and epochs from exploratory with all data for production model\n",
    "\n",
    "all_data = np.concatenate((train_data, val_data), axis=0)\n",
    "all_labels = np.concatenate((train_labels, val_labels), axis=0)\n",
    "model_with_dropout = Sequential()\n",
    "model_with_dropout.add(layers.GRU(32, reset_after=False,  return_sequences=True, input_shape=(None, train_data.shape[-1])))\n",
    "model_with_dropout.add(layers.GRU(64, reset_after=False, dropout=0.2, activation='relu'))\n",
    "model_with_dropout.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model_with_dropout.compile(loss=\"binary_crossentropy\", optimizer=Adam(.0005))\n",
    "history_with_dropout = model_with_dropout.fit(all_data, all_labels, batch_size=2, epochs=28, validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plot_loss(history_with_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make LSTM capable with batch of 1\n",
    "#thanks: https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/\n",
    "model_p = Sequential()\n",
    "model_p.add(layers.GRU(32,  reset_after=False,  return_sequences=True, input_shape=(None, data.shape[-1]))) #interestingly you don't give it all the samples in shape\n",
    "model_p.add(layers.GRU(64, reset_after=False,  activation='relu'))\n",
    "model_p.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "#copy weights from trained model\n",
    "old_weights = model_with_dropout.get_weights()\n",
    "\n",
    "model_p.set_weights(old_weights)\n",
    "model_p.compile(loss=\"binary_crossentropy\", optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some tests\n",
    "wrong = 0\n",
    "right = 0\n",
    "for i in range(2000,10000):\n",
    "    testone = np.expand_dims(scaler.transform(normal_data[i]), axis=0)\n",
    "    # print(ss.shape, ss)\n",
    "    result = model_p.predict(testone)\n",
    "    if result > 0.5:\n",
    "        wrong+=1\n",
    "    else:\n",
    "        right+=1\n",
    "print(wrong, right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_low(sequence):\n",
    "    if(sequence.shape != (24,3)):\n",
    "        raise exception(\"bad sequence\", sequence.shape)\n",
    "    testone = np.expand_dims(scaler.transform(sequence), axis=0)\n",
    "    result = model_p.predict(testone)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_low(get_original_low_data(low_data_map, -3)[:24]))\n",
    "get_original_low_data(low_data_map, -3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the network\n",
    "model_p.save(os.path.join(data_dir,'model_classify_no_reset_after.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the scaler. need to scale new readings on prediction \n",
    "from joblib import dump, load\n",
    "dump(scaler, os.path.join(data_dir,'scaler_model_classify.bin'), compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean: \", scaler.mean_)\n",
    "print(\"var:  \", scaler.var_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
